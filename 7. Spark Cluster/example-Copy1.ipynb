{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from pyspark.sql.types import StructType, StructField, FloatType, BooleanType\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "# Setup the Configuration\n",
    "conf = pyspark.SparkConf()\n",
    "spark_context = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "sqlcontext = SQLContext(sc)\n",
    "# Setup the Schema\n",
    "schema = StructType([\n",
    "StructField(\"User ID\", IntegerType(),True),\n",
    "StructField(\"Username\", StringType(),True),\n",
    "StructField(\"Browser\", StringType(),True),\n",
    "StructField(\"OS\", StringType(),True),\n",
    "])\n",
    "# Add Data\n",
    "data = ([(1580, \"Barry\", \"FireFox\", \"Windows\" ),\n",
    "(5820, \"Sam\", \"MS Edge\", \"Linux\"),\n",
    "(2340, \"Harry\", \"Vivaldi\", \"Windows\"),\n",
    "(7860, \"Albert\", \"Chrome\", \"Windows\"),\n",
    "(1123, \"May\", \"Safari\", \"macOS\")\n",
    "])\n",
    "# Setup the Data Frame\n",
    "user_data_df = sqlcontext.createDataFrame(data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-------+-------+\n",
      "|User ID|Username|Browser|     OS|\n",
      "+-------+--------+-------+-------+\n",
      "|   1580|   Barry|FireFox|Windows|\n",
      "|   5820|     Sam|MS Edge|  Linux|\n",
      "|   2340|   Harry|Vivaldi|Windows|\n",
      "|   7860|  Albert| Chrome|Windows|\n",
      "|   1123|     May| Safari|  macOS|\n",
      "+-------+--------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector.constants import ClientFlag\n",
    "\n",
    "config = {\n",
    "    'user': 'g1u4',\n",
    "    'password': '1Wf5Po',\n",
    "    'port': 30804,\n",
    "    'host': 'f620751b-8a0e-47f1-8625-94a685702999.497129fd685f442ca4df759dd55ec01b.databases.appdomain.cloud',\n",
    "    'ssl_ca': '041baeec-1272-11e9-8c9b-ae2e3a9c1b17'\n",
    "}\n",
    "\n",
    "cnx = mysql.connector.connect(**config)\n",
    "cur = cnx.cursor(buffered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-0dd71019c8c3>:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data_set_initial_6 = pd.read_sql_query(sql, cnx)\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT * FROM grupo1.to_spark\"\n",
    "data_set_initial_6 = pd.read_sql_query(sql, cnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label_hate                          int64\n",
       "likeCount                           int64\n",
       "commentCount                        int64\n",
       "weekday                             int64\n",
       "month                               int64\n",
       "productType_carousel_container      int64\n",
       "productType_clips                   int64\n",
       "productType_feed                    int64\n",
       "productType_igtv                    int64\n",
       "tags_count                          int64\n",
       "score_sentiment                   float64\n",
       "translation_len                     int64\n",
       "language_0                          int64\n",
       "language_1                          int64\n",
       "fk_readability                    float64\n",
       "fk_grade                          float64\n",
       "CARDINAL                          float64\n",
       "GPE                               float64\n",
       "EVENT                             float64\n",
       "FAC                               float64\n",
       "DATE                              float64\n",
       "LANGUAGE                          float64\n",
       "LAW                               float64\n",
       "LOC                               float64\n",
       "MONEY                             float64\n",
       "NORP                              float64\n",
       "ORDINAL                           float64\n",
       "ORG                               float64\n",
       "PERCENT                           float64\n",
       "PERSON                            float64\n",
       "PRODUCT                           float64\n",
       "QUANTITY                          float64\n",
       "TIME                              float64\n",
       "WORK_OF_ART                       float64\n",
       "harmonic_centrality               float64\n",
       "degree_centrality                 float64\n",
       "closeness_centrality              float64\n",
       "Betweenness                       float64\n",
       "cluster_cluster_1                   int64\n",
       "cluster_cluster_2                   int64\n",
       "cluster_cluster_3                   int64\n",
       "cluster_cluster_4                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set_initial_6.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are going to take only the variables we want to the dataset, splitting the features from the label objective\n",
    "\n",
    "data_set_5=data_set_initial_6[[\"label_hate\",\"likeCount\",\"commentCount\",\"weekday\",\"month\",\"productType_carousel_container\",\"productType_clips\",\"productType_feed\",\"productType_igtv\",\"tags_count\",\"score_sentiment\",\"translation_len\",\"language_0\",\"language_1\",\"fk_readability\",\"fk_grade\",\"CARDINAL\",\"GPE\",\"EVENT\",\"FAC\",\"DATE\",\"LANGUAGE\",\"LAW\",\"LOC\",\"MONEY\",\"NORP\",\"ORDINAL\",\"ORG\",\"PERCENT\",\"PERSON\",\"PRODUCT\",\"QUANTITY\",\"TIME\",\"WORK_OF_ART\",\"harmonic_centrality\",\"degree_centrality\",\"closeness_centrality\",\"Betweenness\",\"cluster_cluster_1\",\"cluster_cluster_2\",\"cluster_cluster_3\",\"cluster_cluster_4\"]].copy()\n",
    "features = data_set_5.drop('label_hate', axis=1)\n",
    "labels = data_set_5['label_hate']\n",
    "\n",
    "features_dt_labels=[\"label_hate\",\"likeCount\",\"commentCount\",\"weekday\",\"month\",\"productType_carousel_container\",\"productType_clips\",\"productType_feed\",\"productType_igtv\",\"tags_count\",\"score_sentiment\",\"translation_len\",\"language_0\",\"language_1\",\"fk_readability\",\"fk_grade\",\"CARDINAL\",\"GPE\",\"EVENT\",\"FAC\",\"DATE\",\"LANGUAGE\",\"LAW\",\"LOC\",\"MONEY\",\"NORP\",\"ORDINAL\",\"ORG\",\"PERCENT\",\"PERSON\",\"PRODUCT\",\"QUANTITY\",\"TIME\",\"WORK_OF_ART\",\"harmonic_centrality\",\"degree_centrality\",\"closeness_centrality\",\"Betweenness\",\"cluster_cluster_1\",\"cluster_cluster_2\",\"cluster_cluster_3\",\"cluster_cluster_4\"]\n",
    "features_norm_labels=[\"likeCount\",\"commentCount\",\"weekday\",\"month\",\"productType_carousel_container\",\"productType_clips\",\"productType_feed\",\"productType_igtv\",\"tags_count\",\"score_sentiment\",\"translation_len\",\"language_0\",\"language_1\",\"fk_readability\",\"fk_grade\",\"CARDINAL\",\"GPE\",\"EVENT\",\"FAC\",\"DATE\",\"LANGUAGE\",\"LAW\",\"LOC\",\"MONEY\",\"NORP\",\"ORDINAL\",\"ORG\",\"PERCENT\",\"PERSON\",\"PRODUCT\",\"QUANTITY\",\"TIME\",\"WORK_OF_ART\",\"harmonic_centrality\",\"degree_centrality\",\"closeness_centrality\",\"Betweenness\",\"cluster_cluster_1\",\"cluster_cluster_2\",\"cluster_cluster_3\",\"cluster_cluster_4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 33.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 45.4 MB/s eta 0:00:01     |███████████████▉                | 17.1 MB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.24.1)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[K     |████████████████████████████████| 297 kB 77.3 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.1 scipy-1.10.0 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-abdbb7e2048a>:11: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  train_features_unba.set_axis(features_norm_labels,axis=1,inplace=True)\n",
      "<ipython-input-10-abdbb7e2048a>:19: FutureWarning: DataFrame.set_axis 'inplace' keyword is deprecated and will be removed in a future version. Use `obj = obj.set_axis(..., copy=False)` instead\n",
      "  test_features_unba.set_axis(features_norm_labels,axis=1,inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Normalize for the logistics regression\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#Normalize train features\n",
    "array_support_train_fea = train_features.values #returns a numpy array\n",
    "min_max_scaler_lr_data_set = preprocessing.MinMaxScaler()\n",
    "train_fea_data_scaled = min_max_scaler_lr_data_set.fit_transform(array_support_train_fea)\n",
    "train_features_unba = pd.DataFrame(train_fea_data_scaled)\n",
    "train_features_unba.set_axis(features_norm_labels,axis=1,inplace=True)\n",
    "train_features_unba\n",
    "\n",
    "#Normalize test features\n",
    "array_support_test_fea = test_features.values #returns a numpy array\n",
    "min_max_scaler_lr_data_set_test_f = preprocessing.MinMaxScaler()\n",
    "test_fea_data_scaled = min_max_scaler_lr_data_set_test_f.fit_transform(array_support_test_fea)\n",
    "test_features_unba = pd.DataFrame(test_fea_data_scaled)\n",
    "test_features_unba.set_axis(features_norm_labels,axis=1,inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /usr/local/lib/python3.8/dist-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.8/dist-packages (from imblearn) (0.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.24.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.10.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.8/dist-packages (from imbalanced-learn->imblearn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 835, 1: 5})\n",
      "Counter({0: 275, 1: 6})\n",
      "The class we want to predict is unbalanced the prediction models had to be Smote to be able to learn\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler, NearMiss\n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(train_labels))\n",
    "print(Counter(test_labels))\n",
    "print(f'The class we want to predict is unbalanced the prediction models had to be Smote to be able to learn')\n",
    "\n",
    "#Source: https://medium.com/grabngoinfo/four-oversampling-and-under-sampling-methods-for-imbalanced-classification-using-python-7304aedf9037\n",
    "#Source: https://towardsdatascience.com/oversampling-and-undersampling-5e2bbaf56dcf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unbalanced Data\n",
    "train_features['label']=train_labels.tolist()\n",
    "test_features['label']=test_labels.tolist()\n",
    "\n",
    "training_df=train_features.copy()\n",
    "test_df=test_features.copy()\n",
    "\n",
    "    #For Logistics Regression\n",
    "train_features_unba['label']=train_labels.tolist()\n",
    "test_features_unba['label']=test_labels.tolist()\n",
    "\n",
    "training_df_lr=train_features_unba.copy()\n",
    "test_df_lr=test_features_unba.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- likeCount: long (nullable = true)\n",
      " |-- commentCount: long (nullable = true)\n",
      " |-- weekday: long (nullable = true)\n",
      " |-- month: long (nullable = true)\n",
      " |-- productType_carousel_container: long (nullable = true)\n",
      " |-- productType_clips: long (nullable = true)\n",
      " |-- productType_feed: long (nullable = true)\n",
      " |-- productType_igtv: long (nullable = true)\n",
      " |-- tags_count: long (nullable = true)\n",
      " |-- score_sentiment: double (nullable = true)\n",
      " |-- translation_len: long (nullable = true)\n",
      " |-- language_0: long (nullable = true)\n",
      " |-- language_1: long (nullable = true)\n",
      " |-- fk_readability: double (nullable = true)\n",
      " |-- fk_grade: double (nullable = true)\n",
      " |-- CARDINAL: double (nullable = true)\n",
      " |-- GPE: double (nullable = true)\n",
      " |-- EVENT: double (nullable = true)\n",
      " |-- FAC: double (nullable = true)\n",
      " |-- DATE: double (nullable = true)\n",
      " |-- LANGUAGE: double (nullable = true)\n",
      " |-- LAW: double (nullable = true)\n",
      " |-- LOC: double (nullable = true)\n",
      " |-- MONEY: double (nullable = true)\n",
      " |-- NORP: double (nullable = true)\n",
      " |-- ORDINAL: double (nullable = true)\n",
      " |-- ORG: double (nullable = true)\n",
      " |-- PERCENT: double (nullable = true)\n",
      " |-- PERSON: double (nullable = true)\n",
      " |-- PRODUCT: double (nullable = true)\n",
      " |-- QUANTITY: double (nullable = true)\n",
      " |-- TIME: double (nullable = true)\n",
      " |-- WORK_OF_ART: double (nullable = true)\n",
      " |-- harmonic_centrality: double (nullable = true)\n",
      " |-- degree_centrality: double (nullable = true)\n",
      " |-- closeness_centrality: double (nullable = true)\n",
      " |-- Betweenness: double (nullable = true)\n",
      " |-- cluster_cluster_1: long (nullable = true)\n",
      " |-- cluster_cluster_2: long (nullable = true)\n",
      " |-- cluster_cluster_3: long (nullable = true)\n",
      " |-- cluster_cluster_4: long (nullable = true)\n",
      " |-- label: long (nullable = true)\n",
      "\n",
      "+---------+------------+-------+-----+------------------------------+-----------------+----------------+----------------+----------+------------------+---------------+----------+----------+--------------+--------+--------+---+-----+---+----+--------+---+---+-----+----+-------+---+-------+------+-------+--------+----+-----------+-------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----+\n",
      "|likeCount|commentCount|weekday|month|productType_carousel_container|productType_clips|productType_feed|productType_igtv|tags_count|   score_sentiment|translation_len|language_0|language_1|fk_readability|fk_grade|CARDINAL|GPE|EVENT|FAC|DATE|LANGUAGE|LAW|LOC|MONEY|NORP|ORDINAL|ORG|PERCENT|PERSON|PRODUCT|QUANTITY|TIME|WORK_OF_ART|harmonic_centrality| degree_centrality|closeness_centrality|       Betweenness|cluster_cluster_1|cluster_cluster_2|cluster_cluster_3|cluster_cluster_4|label|\n",
      "+---------+------------+-------+-----+------------------------------+-----------------+----------------+----------------+----------+------------------+---------------+----------+----------+--------------+--------+--------+---+-----+---+----+--------+---+---+-----+----+-------+---+-------+------+-------+--------+----+-----------+-------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----+\n",
      "|       35|           1|      6|    6|                             0|                0|               1|               0|         0|0.9998745918273926|             38|         0|         1|         61.24|     8.0|     0.0|0.0|  0.0|0.0| 0.0|     0.0|0.0|0.0|  0.0| 0.0|    0.0|0.0|    0.0|   0.0|    0.0|     0.0| 0.0|        0.0|  33.52385770069475|0.0122426797536312|  0.0279486917640702|216.33275440015785|                0|                0|                1|                0|    0|\n",
      "|     7168|          49|      4|    2|                             0|                0|               1|               0|         0| 0.998425841331482|              5|         1|         0|         36.62|    10.0|     0.0|0.0|  0.0|0.0| 0.0|     0.0|0.0|0.0|  0.0| 0.0|    0.0|0.0|    0.0|   0.0|    0.0|     0.0| 0.0|        0.0|  33.52385770069475|0.0122426797536312|  0.0279486917640702|216.33275440015785|                0|                0|                1|                0|    0|\n",
      "+---------+------------+-------+-----+------------------------------+-----------------+----------------+----------------+----------+------------------+---------------+----------+----------+--------------+--------+--------+---+-----+---+----+--------+---+---+-----+----+-------+---+-------+------+-------+--------+----+-----------+-------------------+------------------+--------------------+------------------+-----------------+-----------------+-----------------+-----------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Next we will be passing the pandas dataframe to Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"pandas to spark\").getOrCreate()\n",
    "  \n",
    "#probabily need to install Java if you do not have it and create dependencies consult the next website to have an ideia: https://www.computerhope.com/issues/ch000549.htm#windows10 and https://stackoverflow.com/questions/56213955/python-worker-failed-to-connect-back-in-pyspark-or-spark-version-2-3-1\n",
    "sparkDF_train=spark_context.createDataFrame(training_df) \n",
    "sparkDF_test=spark_context.createDataFrame(test_df) \n",
    "\n",
    "#Now the dataset corrected to lr\n",
    "sparkDF_train_lr=spark_context.createDataFrame(training_df_lr) \n",
    "sparkDF_test_lr=spark_context.createDataFrame(test_df_lr) \n",
    "\n",
    "\n",
    "sparkDF_test.printSchema()\n",
    "sparkDF_test.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'str'>\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(41,[0,1,2,3,6,9,...|    0|\n",
      "|(41,[0,1,2,3,6,9,...|    0|\n",
      "|(41,[0,1,2,3,6,8,...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "Duration: 0:00:02.891460\n"
     ]
    }
   ],
   "source": [
    "#Next will have to addopt the data set to be able to properly split and train the models\n",
    "#Used the defined feature labels above and the target labels above\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "# do your work here\n",
    "\n",
    "\n",
    "\n",
    "labels = training_df['label']\n",
    "\n",
    "feature_headers = list(features.columns.values)\n",
    "label_headers = labels.name\n",
    "\n",
    "#\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols = feature_headers, outputCol='features')\n",
    "\n",
    "trainingData = va.transform(sparkDF_train)\n",
    "testData = va.transform(sparkDF_test)\n",
    "\n",
    "trainingData = trainingData.select(['features', label_headers])\n",
    "testData = testData.select(['features', label_headers])\n",
    "\n",
    "#For Logistics Regression\n",
    "trainingData_lr = va.transform(sparkDF_train_lr)\n",
    "testData_lr = va.transform(sparkDF_test_lr)\n",
    "\n",
    "trainingData_lr = trainingData_lr.select(['features', label_headers])\n",
    "testData_lr = testData_lr.select(['features', label_headers])\n",
    "\n",
    "print(type(feature_headers))\n",
    "print(type(label_headers))\n",
    "\n",
    "#Source: https://www.datatechnotes.com/2021/06/pyspark-decision-tree-classification.html\n",
    "trainingData.show(3)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "|            features|label|rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "|(41,[0,1,2,3,6,9,...|    0|   [48.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "|(41,[0,1,2,3,6,9,...|    0|  [737.0,1.0]|[0.99864498644986...|       0.0|\n",
      "|(41,[0,1,2,3,4,8,...|    0|   [48.0,0.0]|           [1.0,0.0]|       0.0|\n",
      "+--------------------+-----+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Duration: 0:00:05.715627\n"
     ]
    }
   ],
   "source": [
    "#Next we will be defing the decision tree\n",
    "start_time = datetime.now()\n",
    "\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing) We are not using the below function due to the random split it performs\n",
    "#(trainingData, testData) = va_df.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "dtc = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "dtc = dtc.fit(trainingData)\n",
    "\n",
    "# After training we will be testing out dataset\n",
    "pred = dtc.transform(testData)\n",
    "pred.show(3)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[274   1]\n",
      " [  5   1]]\n"
     ]
    }
   ],
   "source": [
    "# After training the model, we'll predict test data and check the accuracy metrics. Here, we can use MulticlassClassificationEvaluator to check the accuracy. \n",
    "#Confusion matrix can be created by using confusion_matrix function of sklearn.metrics module.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "evaluator=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "#print(\"Prediction Accuracy: \", acc)\n",
    "\n",
    "y_pred=pred.select(\"prediction\").collect()\n",
    "y_orig=pred.select(\"label\").collect()\n",
    "\n",
    "cm = confusion_matrix(y_orig, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "#Define the Evaluation Metrics\n",
    "#Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "accu_dt = evaluator.evaluate(pred)\n",
    "#The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "prec_dt=cm[1,1]/(cm[1,1]+cm[1,0])\n",
    "\n",
    "#The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "recall_dt=cm[1,1]/(cm[1,1]+cm[0,1])\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "f1_dt=2*((prec_dt*recall_dt)/(prec_dt+recall_dt))\n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics_dt = BinaryClassificationEvaluator()\n",
    "auc_dt=metrics_dt.evaluate(pred, {metrics_dt.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(41,[0,1,2,3,6,9,...|    0|[19.9517724832183...|[0.99758862416091...|       0.0|\n",
      "|(41,[0,1,2,3,6,9,...|    0|[19.9515663698298...|[0.99757831849149...|       0.0|\n",
      "|(41,[0,1,2,3,4,8,...|    0|[19.9583103690665...|[0.99791551845332...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Duration: 0:00:02.369142\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "start_time = datetime.now()\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "rfc = rfc.fit(trainingData)\n",
    "\n",
    "# After training we will be testing out dataset\n",
    "pred_rf = rfc.transform(testData)\n",
    "pred_rf.show(3)\n",
    "\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[275   0]\n",
      " [  6   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-adf441fe06ec>:31: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  recall_rf=cm_rf[1,1]/(cm_rf[1,1]+cm_rf[0,1])\n"
     ]
    }
   ],
   "source": [
    "# After training the model, we'll predict test data and check the accuracy metrics. Here, we can use MulticlassClassificationEvaluator to check the accuracy. \n",
    "#Confusion matrix can be created by using confusion_matrix function of sklearn.metrics module.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "evaluator_rf=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "#acc = evaluator.evaluate(pred)\n",
    "\n",
    "#print(\"Prediction Accuracy: \", acc)\n",
    "\n",
    "y_pred_rf=pred_rf.select(\"prediction\").collect()\n",
    "y_orig_rf=pred_rf.select(\"label\").collect()\n",
    "\n",
    "cm_rf = confusion_matrix(y_orig_rf, y_pred_rf)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_rf)\n",
    "\n",
    "#Define the Evaluation Metrics\n",
    "#Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "accu_rf = evaluator_rf.evaluate(pred_rf)\n",
    "#The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "prec_rf=cm_rf[1,1]/(cm_rf[1,1]+cm_rf[1,0])\n",
    "\n",
    "#The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "recall_rf=cm_rf[1,1]/(cm_rf[1,1]+cm_rf[0,1])\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "f1_rf=2*((prec_rf*recall_rf)/(prec_rf+recall_rf))\n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics_rf = BinaryClassificationEvaluator()\n",
    "auc_rf=metrics_rf.evaluate(pred_rf, {metrics_rf.metricName: 'areaUnderROC'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistics Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(41,[0,1,2,3,6,9,...|    0|[-95.817724111312...|[2.43719989043571...|       1.0|\n",
      "|(41,[0,1,2,3,6,9,...|    0|[-91.510458726808...|[1.80930897923491...|       1.0|\n",
      "|(41,[0,1,2,3,4,8,...|    0|[-65.315075205094...|[4.30549662698323...|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Duration: 0:00:08.512462\n"
     ]
    }
   ],
   "source": [
    "# Train a Logistics Regression\n",
    "#  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "start_time = datetime.now()\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lrc = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lrc = lrc.fit(trainingData_lr)\n",
    "\n",
    "# After training we will be testing out dataset\n",
    "pred_lr = lrc.transform(testData_lr)\n",
    "pred_lr.show(3)\n",
    "\n",
    "end_time = datetime.now()\n",
    "\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 51 224]\n",
      " [  1   5]]\n"
     ]
    }
   ],
   "source": [
    "# After training the model, we'll predict test data and check the accuracy metrics. Here, we can use MulticlassClassificationEvaluator to check the accuracy. \n",
    "#Confusion matrix can be created by using confusion_matrix function of sklearn.metrics module.\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "evaluator_lr=MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "#acc = evaluator.evaluate(pred)\n",
    "\n",
    "#print(\"Prediction Accuracy: \", acc)\n",
    "\n",
    "y_pred_lr=pred_lr.select(\"prediction\").collect()\n",
    "y_orig_lr=pred_lr.select(\"label\").collect()\n",
    "\n",
    "cm_lr = confusion_matrix(y_orig_lr, y_pred_lr)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_lr)\n",
    "\n",
    "#Define the Evaluation Metrics\n",
    "#Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "accu_lr = evaluator_lr.evaluate(pred_lr)\n",
    "#The precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
    "prec_lr=cm_lr[1,1]/(cm_lr[1,1]+cm_lr[1,0])\n",
    "\n",
    "#The recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
    "recall_lr=cm_lr[1,1]/(cm_lr[1,1]+cm_lr[0,1])\n",
    "\n",
    "#The F1 score can be interpreted as a harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is:\n",
    "f1_lr=2*((prec_lr*recall_lr)/(prec_lr+recall_lr))\n",
    "#F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Instantiate metrics object\n",
    "metrics_lr = BinaryClassificationEvaluator()\n",
    "auc_lr=metrics_lr.evaluate(pred_lr, {metrics_lr.metricName: 'areaUnderROC'})\n",
    "#Source: https://towardsdatascience.com/binary-classifier-evaluation-made-easy-with-handyspark-3b1e69c12b4f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Logistics Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.973387</td>\n",
       "      <td>0.968087</td>\n",
       "      <td>0.306175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1_score</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.042553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.469394</td>\n",
       "      <td>0.595152</td>\n",
       "      <td>0.634545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Decision Tree  Random Forest  Logistics Regression\n",
       "Accuracy        0.973387       0.968087              0.306175\n",
       "Precision       0.166667       0.000000              0.833333\n",
       "Recall          0.500000            NaN              0.021834\n",
       "F1_score        0.250000            NaN              0.042553\n",
       "AUC             0.469394       0.595152              0.634545"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_results=pd.DataFrame(\n",
    "    [\n",
    "        [accu_dt,accu_rf,accu_lr],\n",
    "        [prec_dt,prec_rf,prec_lr],\n",
    "        [recall_dt,recall_rf,recall_lr],\n",
    "        [f1_dt,f1_rf,f1_lr],\n",
    "        [auc_dt,auc_rf,auc_lr]\n",
    "    ],\n",
    "    columns=[\"Decision Tree\",\"Random Forest\",\"Logistics Regression\"],\n",
    "    index=[\"Accuracy\",\"Precision\",\"Recall\",\"F1_score\", \"AUC\"])\n",
    "spark_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
